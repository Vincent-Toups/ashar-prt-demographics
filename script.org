
#+TITLE: Notes for the BACPAC Data Portal Workshop
#+AUTHOR: Vincent Toups
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css" />

#+INCLUDE: schedule.org
* Introduction

Hi everyone. Thank you for making time to attend this small
workshop/hackathon to introduce the BACPAC Data
Portal.

Today we'll cover:

   1. Getting access to the data portal. (Although you hopefully did
      this before the workshop)
   2. Using the web based Data Portal to browse for assets and upload
      personal files.
   3. Starting and connecting to your virtual machines and doing a
      little analysis.
   4. Installing unusual software or libraries in your virtual
      machines
   5. Using Docker containers in the case that all else fails.

There will be a few points during the workshop where I'll as
participants to spend some time actually using the portal so that you
get go through some of the more technical parts of the process
yourself while I'm here to assist.

To give you all an excuse to do more than a little work on the portal,
we'll meet again in one week for people to share some interesting
visualizations of the Ashar data set.

* Gaining Access to the BACPAC Data Portal.

The key to accessing the data portal is the "Data Access and
Publication Request Form" which you or someone from your research
group will fill out. After you submitted your DAPR requests, you
should have been added to a project called "~Workshop_April2022~", which
will give you access to the data set we will be using in this project
(the data set associated with the Ashar et al 2021 paper "Effect of
Pain Reprocessing Therapy vs Placebo and Usual Care for Patients With
Chronic Back Pain A Randomized Clinical Trial").

The [[https://unc.az1.qualtrics.com/CP/File.php?F=F_eVy3GPRAKK2sTFY][form]] is available via Microsoft Teams and needs to be submitted
via [[https://teams.microsoft.com/l/channel/19%3A1f17988f3f9d47019b45e7221b449e14%40thread.skype/tab%3A%3Aaf358994-7635-4498-af88-bc767bd20348?groupId=6633d7f4-6063-419d-ba64-99e9b9e87e75&tenantId=58b3d54f-16c9-42d3-af08-1fcabd095666][Teams]].

You should all have filled this form out before this meeting and
already have data portal access and access to the ~Workshop_April2022~
Project. If not, you are welcome to follow along, but you may not get
the most of the workshop without putting your hands on the Data Portal.

* The BACPAC Data Portal

Visit [[https://www.bacpacresearch.org][www.bacpacresearch.org]] and click log in. Here you log in with
whatever email address you used when making your DAPR request.

Then we're in.

#+CAPTION: The BACPAC Data Portal Welcome Page
#+NAME: welcome
[[./script-images/welcome.png]]

There is a lot going on with the data portal, so let's do a quick
run through.

The fundamental reason the data portal exists is to provide
researchers with a place to access data which cannot be readily
shared for confidentiality and privacy reasons. Instead of bringing
the data to your compute resources, the Data Portal lets you create
and work on compute resources which are next to the data.

In order to do that, each data portal account is associated with a
special profile. Once VMs are created on the Portal's web interface,
you can use that profile to log into the machines via your browser
and work from there.

#+CAPTION: Your profile page contains information necessary to log into your BACPAC Virtual Machines.
#+NAME: profile-page-ss-new
[[./script-images/profile-page-ss-new.png]]

** The Asset Browser

To facilitate asset sharing, the data portal provides an asset browser
which allows you to search for data available on the portal in a
variety of ways.

A variety of standard meta-data is available for each data set. In
addition, many data sets have custom meta-data fields which are
controlled by the DAC and allow you to search (for instance) for any data
set which contains a particular column or column value (where
confidentially allows us to expose that information).

This meta-data search feature can be a little tricky to understand but
is pretty useful.

The technically savvy among us might want to think of a data-set's
meta-data as a JSON object with a set of Key/value pairs, where most
often the value of which is a list.

If you submit data to the Data Portal you may wish to let me know how
best to generate meta-data or to send the meta-data fields you'd like
to make searchable to me.

** Project Space

Each Data Access and Publication Request Form results in the creation
of a new project (project names are assigned by Cameron Gunn during
processing based on the information in the form) under the assumption
that each team implied in a new form (as opposed to a request for
modification) constitutes a new project.

These project spaces are a great place to host a _centralized_ git
repository, particularly because the VM firewall prevents access to
sites like github which might otherwise host git repositories. I'll
walk you through the process later.

** My Workspace

This is personal space. We'll see several uses for this space
throughout the workshop, but you can think of it as the "approved"
channel between your virtual machines and the outside world.

If you draft a paper inside your VM and want to download a PDF, then
you would put it in this space and Download it from the Data
Portal. If you want to upload an R library not available on CRAN, or a
set of SAS utilities that you carry around with you, you would upload
it to this space and it will be available within the VM.

** The Computing Console

Since this is a hackathon-type event lets just jump right into the
computing console.

#+CAPTION: The computing console.
#+NAME: compute-console
[[./script-images/compute-console.png]]

When you first log on you'll see that you have no machines of any
type associated with your account. I'm primarily a Linux users but
lets begin by starting a Windows VM (a small one). The plan here is
for us to start the VM, explore a data set, and then install some
custom software.

Click the "+" button next to the "Computing Console" title:

#+CAPTION: The "+" button allows you to create a new virtual machine.
#+NAME: the-plus-button
[[./script-images/the-plus-button.png]]


We have our choice of many virtual machines. Unless you have a
specific high performance computing task, its most cost effective
for BACPAC if you run a small VM. Let's do that now.

After waiting a little while our VM will be ready to go. Our login
process is a bit complicated, but let's go ahead and walk through
it.

We click the VM, copy the link from the connect button, paste it
into an incognito window, open our profile or VM page, and log in a
few times with the password from our profile page (this password is
also on each VM page, for convenience). Eventually we will find ourselves
looking at a Windows Desktop.

#+CAPTION: VMs use a different username and password which is presented here.
#+NAME: vm-user-and-password
[[./script-images/vm-user-and-password.png]]


#+CAPTION: No one knows.
#+NAME: dog
[[./script-images/dog.png]]

#+CAPTION: WVD (windows Virtual Desktop), your gateway to the BACPAC Virtual Machines.
#+NAME: rd-web-opening-page
[[./script-images/rd-web-opening-page.png]]

We click "remote desktop" and then log in:

#+CAPTION: Log in with your special "profile" credentials.
#+NAME: log-in-1
[[./log-in-1.png]]

Click "show options"

#+CAPTION: Click "show options."
#+NAME: show-options
[[./script-images/show-options.png]]

The "Open" to open a pre-configured connection setup.
#+CAPTION: Click "open."
#+NAME: open-a-connection-configuration
[[./script-images/open-a-connection-configuration.png]]

And then select the right configuration for you virtual machine. Its
pretty straightforward: PRD-<OS>-<Number>-<Size>.

#+CAPTION: Opening a medium (M) windows (WIN) machine.
#+NAME: opening-a-medium-windows-machine
[[./script-images/opening-a-medium-windows-machine.png]]

After clicking "open" we can now click connect:

#+CAPTION: Clicking connect.
#+NAME: clicking-connect
[[./script-images/clicking-connect.png]]

And we have a few more hoops to jump through:

#+CAPTION: Click "connect."
#+NAME: accept-security-warning
[[./script-images/accept-security-warning.png]]

Enter your password again:

#+CAPTION: Enter your password again.
#+NAME: password-again
[[./script-images/password-again.png]]

Accept the certificate:

#+CAPTION: Accept the certificate.
#+NAME: accept-certificate
[[./script-images/accept-certificate.png]]

And you will be connected to your Virtual Machine's desktop.

** Tour of the VM

Regardless of the size of the VM you started, the setup of the Windows
VM will be similar. The most important element is the location of the
data. Let's navigate to ~C:/mnt/containers/~ to see our personal,
project, and canonical data. For this tutorial we'll be working with
the Ashar data in the project space created for this workshop.

Things should more or less work as you expect in this VM. Note,
however, that most of the web is blocked to make accidentally
leaking confidential data more difficult.

There are exceptions to this rule meant to make life easier: CRAN
and PIP repositories are unlocked so you can install material from
them (assuming they don't require any other access to the internet).

For example, it isn't a standard part of the VM load out, but many
users enjoy using Jupyter notebooks to organize their work. We can
install Jupyter like this:

#+CAPTION: Using pip on Windows to install Jupyterlab.
#+NAME: pip-windows
[[./script-images/pip-windows.png]]


Or on a Linux machine:

#+CAPTION: Installing Jupyter Lab on Linux.
#+NAME: jupyter-on-linux
[[./script-images/jupyter-on-linux.png]]

On either system we start Jupyter by running:

#+begin_src bash
  jupyter lab
#+end_src

#+CAPTION: Jupyter Lab up and running.
#+NAME: linux-tada
[[./script-images/linux-tada.png]]

** Data

The main point of the Data Portal is data analysis. The data portal
separates its data sets into three categories: Canonical data contains
site-specific data which is, by default, accessible only to your
research site, Project Data (which only people who are
part of the project may access) and Personal Data which you may use
to move things in and out of the data portal.

On both linux and Windows these files are mounted in (more or less)
the same location:

~/mnt/containers/~

or

~C:\mnt\containers\~

Within these directories are the personal, project, and canonical
spaces.

If you upload data to your personal space, it will appear in the
folder under personal. You get the idea.

For today's workshop we'll be looking at the Ashar data set which is
available in a shared project folder we created explicitly for the
workshop:

#+begin_src fundamental
  C:\mnt\containters\project\Workshop_April2022/ashar_data
  # or
  /mnt/containers/project/Workshop_April2022/ashar_data
#+end_src

#+CAPTION: The location of the Ashar et al data set in the project space.
#+NAME: project-ashar-location
[[./script-images/project-ashar-location.png]]

** Doing something w/ the Data
** "Effect of Pain Reprocessing Therapy vs Placebo and Usual Care for Patients With Chronic Back Pain"

Everyone at the workshop should feel free to explore the data portal
with whatever data set they wish, but so that we can all have a common
baseline I'll be using data from the paper "Effect of Pain Reprocessing Therapy vs Placebo
and Usual Care for Patients With Chronic Back Pain" (Ashar et al, JAMA
Psychiatry, 2021).

Very briefly, this paper examines the efficacy of Pain
Reprocessing Therapy for chronic lower back pain (compared to
saline injections and standard of care).

Patients were split into three groups (Pain Reprocessing
Therapy (PRT), Saline Injection and standard of care) and tracked for
12 months. The results are very positive for PRT:

#+CAPTION: A pretty obvious success story.
#+NAME: effectiveness-of-prt-on-intensity
[[./script-images/effectiveness-of-prt-on-intensity.png]]

Let's take a deeper look at the data.

If we view the folder where our data is located:

#+CAPTION: The Ashar data set and associated meta-data.
#+NAME: ashar-dataset-location-windows
[[./script-images/ashar-dataset-location-windows.png]]

We can see that the data set comes with a codebook. We'll mostly be
working with the ~clinical_outcomes.csv~ data set here, and it turns
out that the information about this data set is in "Codebook
Additional notes.txt":

#+CAPTION: The clinical outcomes codebook.
#+NAME: codebook-ashar
[[./script-images/codebook-ashar.png]]

Let's use Windows and our newly installed Jupyter server to look at
our Ashar data set.

We'll be looking at the clinical outcomes data set from that
repository.

#+CAPTION: The first few rows and columns of the Ashar clinical outcomes data set.
#+NAME: ashar-clinical-outcomes-ss
[[./script-images/ashar-clinical-outcomes-ss.png]]


The simplest thing for us to do here is to load the data and make a
set of pairplots.

#+CAPTION: A subset of the pairplots from the Ashar clinical outcomes dataset.
#+NAME: ashar-clinical-outcomes
[[./script-images/ashar-clinical-outcomes.png]]

** Portable Paths

Its worth taking a bit of time to consider making code which loads
from the container space portable over Windows and Linux. Here is what
that looks like in R:

#+begin_src R
  container_path <- function(container, path){
      platform <- .Platform$OS.type;
      if(platform == "windows"){
          paste("C:/mnt/containers/",container,"/",paste(path,collapse=.Platform$file.sep),sep="")
      } else if (platform == "unix") {
          paste("/mnt/containers/",container,"/",paste(path,collapse=.Platform$file.sep),sep="")
      }
  }  
#+end_src

#+begin_src fundamental
  # on linux
  [1] "/mnt/containers/project/Workshop_April2022/ashar_data"
  # on windows
  [1] "C:\mnt\containers\project\Workshop_April2022\ashar_data"
#+end_src

We'd use this like:

#+begin_src R
  library(tidyverse)
  data <- read_csv(container_path("project", c("Workshop_April2022","ashar_data",
                                               "clinical_outcomes.csv")))
#+end_src

And that would work on both Windows and Linux.

* Work Time

At this point I'd like to take a half hour to let participants log
into the Data Portal, build some VM's, log into them, and at least
open the data and look at it in their preferred environment. I'll be
available the entire time for questions. This is also a good time to
stretch your legs, make some tea, run a quick mile.

#+CAPTION: Try it out for yourself.
#+NAME: work-time
[[./script-images/work-time.png]]

* Installing External Software on your VMs

The VM's are totally locked down (with the exception of pip and CRAN
and a few other sites). So if you're like me and you want to
program in J, you have to install this software on your own. The
general idea here is:

1. download the installer/package/whatever for your system
2. upload it to your VM
3. install the software there

We'll try several different pieces of software just so you get a sense
for the process.

** An R Package not on CRAN

The Ashar Data includes MRI data in the [[https://nifti.nimh.nih.gov/nifti-1/][nifty format]]. We can work with
this format using an R Package that is available on CRAN, but we can
assume for the purposes of demonstration that we need the latest
version of the package, which hasn't yet been accepted.

The code is managed on Github but if we try to visit Github from our
VM we see:

#+CAPTION: We cannot access github from within the VM.
#+NAME: github-is-blocked
[[./script-images/github-is-blocked.png]]

The process is simple enough:

#+CAPTION: Click code and then "Download ZIP".
#+NAME: download-zip-nifty
[[./script-images/download-zip-nifty.png]]

And then we can upload that file to the Data Portal.

Once it is there we can install it with R using ~devtools~.

On Windows the process is a little more complicated. The default
Windows installation of R doesn't include the necessary software to
install packages that require a compiler. But we can solve that
problem.  We just download RTools, upload it to our VM, and install
it.

** A Utility Program

Another nice example is that we might want to use a MRI Tool. The FSL
documentation mentions a tool called MRIcron which we can download
[[https://www.nitrc.org/frs/?group_id=152][here]]. This is an easy one: its a stand alone tool on linux, so we juts
upload it, unzip it and run it.

** An entire Analysis System

*** A Brief Aside about Ursala

#+CAPTION: Ursala may be used to instruct a computer to perform a task.
#+NAME: may-be-used
[[./script-images/may-be-used.png]]

Many years ago I was browsing Reddit when I encountered the following
post: "The Ursala Programming Language: More Power, Less Verbiage"

You can imagine my confusion when I clicked through and saw someone
confidently espousing the value of their own home grown programming
language that looked like this:

#+CAPTION: Ursala - Thanks but no thanks.
#+NAME: thanks-but-no
[[./script-images/thanks-but-no.png]]

Ursala, I would later learn, is one of the many descendants of the
programming language [[https://en.wikipedia.org/wiki/APL_(programming_language)][APL]].

#+CAPTION: An APL Keyboard.
#+NAME: an-apl-keyboard
[[./script-images/an-apl-keyboard.png]]

Back when people still programmed in Perl, other people used to joke
that Perl looked like "line noise" - random characters you'd get if
you just send random fluctuations down a teletype terminal. Despite
the fact that Ursala looks a lot more like line noise than APL, I was
intrigued. In fact, I was already programming in Matlab, which, like
R and Numpy, are descendants of APL, and by pulling on the thread that
Ursala showed me, I eventually became interested in J.

And now I will pass that brain virus onto you.

The good ideas in J:

1. tacit programming - If an expression involves only functions (verbs
   in J) then that function "automagically" composes into a more
   complex function according to a few rules. This is a little mind
   bending but its sort of like "data flow" programming - you don't
   name things you operate on, just think about flow of data.
2. rank - verbs have "rank" (sort of like multidimensional matrices)
   that you can customize on the fly. This effects the way that verbs
   operate on the matrices: rank 0 verbs operate on the whole thing,
   rank 1 on the "1 cells" (eg, in a 1 dimensional array the elements,
   in a two dimensional array the rows, etc). Using rank you can
   eliminate almost all looping constructs.

The bad ideas in J:

1. everything else

But in practice making any specific piece of software work the way you
want can be a little tricky. Setting up J on the Windows VM is just
slightly non-trivial and so serves as a good example case.

The first step is to download J for Windows. If possible, you want to
download a "standalone" version of the software you need instead of an
installer. Its not uncommon for installers to want to talk to the
internet, and this is a non-starter.

If we visit [[https://jsoftware.com][JSoftware]] we can see what sort of installers we need.

J happens to have a zip "stand alone" type installer which we can grab
here:

#+CAPTION: Just a directory of files.
#+NAME: j-software-zip
[[./script-images/j-software-zip.png]]

We now just unpack this locally and follow the instructions. J is nice
in that you can install literally every package that it comes with in
a few minutes:

#+begin_src J
  load 'pacman'
  'install' jpkg '*'
  exit 0
#+end_src

So I've done this already on my CSCC Desktop (which runs windows) and
then copied it into my VM.

#+CAPTION: Uploading a zip of my J directories.
#+NAME: upload-j908-bundle
[[./upload-j908-bundle.png]]


Once I've got it into the VM, I can just start J and do some analysis:

#+CAPTION: The Ashar data plotted with J.
#+NAME: ashar-lineplot-from-j
[[./script-images/ashar-lineplot-from-j.png]]

Believe it or not, this is unusually comprehensible J.

* git On the Data Portal VMs
** What/Why is Git

Git is a version control system that many people use to organize their
software development. It is also very useful for scientific analysis,
since it can allow you to maintain a (very granular) history of the
analysis code.

#+CAPTION: Git was originally intended for fully distributed version control.
#+NAME: distributed-version-control
[[./script-images/distributed-version-control.png]]

But in practices most projects have a central repository and we will
too.

#+CAPTION: The contemporary way things work.
#+NAME: pseudo-distributed-vc
[[./script-images/pseudo-distributed-vc.png]]

The absolute least you need to know to use git are these commands:

#+begin_src bash
  git init # create a repository
  git add <filename> # tell git you would like to record changes to
  # this file in the next commmit
  # you may make several "git adds" before a ...
  git commit -m "I changed a file (ideally more explicit commit message)"
  # record the change you "staged" above.
  git push # send your commits to a remote repository
  git fetch # fetch commits (perhaps made by others) into your local repository
  git rebase <remote-name>/main
  # try to integrate the fetched commits just fetched into your own code in a "polite" way.
#+end_src

I'd encourage everyone to use git if they working with code on the
BACPAC Virtual Machines. Most people who want to share code these days
use [[https://github.com][github]] for sharing repositories, but github is blocked by our
firewall.

The most reasonable way for users to share a common git repository is
to create a repository on their shared project space. This can be done
from either Linux or Windows and only needs to be done once:
** Linux Vs Windows

Git's origin goes all the way back to the Linux kernel. It was
developed when the VCS that Linux had been using with a free license
revoked that right over a misunderstanding about reverse
engineering. Linus Torvalds developed git concurrently with Linux to
serve as the version controls system for that project.

Thus, git is "linux centric." But so great is its utility that
affordances have been made to use the platform on Windows as well. You
have a few options. Users on windows can use `git bash` which gives
you a simulated Linux-like command line where you can run git
commands.

Just right click somewhere in the repo folder and select "Git Bash
here."

** Setting up a Git Workflow on the VMS

#+CAPTION: Open a git bash shell in Windows.
#+NAME: git-bash-here
[[./script-images/git-bash-here.png]]

Git GUI may be more comfortable for Windows users, but at present we
are working on a bug which prevents it form working on the VMs.

#+begin_src bash
cd /c/mnt/containers/project/Workshop_April2022/
git init --bare --initial-branch main my_cool_project
#+end_src

#+CAPTION: Initializing a bare git repository on Windows.
#+NAME: init-bare-windows
[[./script-images/init-bare-windows.png]]

The "lead" of the project will typically initialize such a
repository. That person, as well as everyone else who will contribute
to the project, will clone this repository into their own personal
space.

That looks like this:

#+begin_src bash
  cd /mnt/containers/personal/vincentt51822/
  git clone /c/mnt/containers/project/Workshop_April2022/my_cool_project
#+end_src

Once we've cloned the repository we can add some stuff to it and make
a commit.

#+begin_src bash
  cd my_cool_project
  touch README.md
  git add README.md
  git commit -m "I added (an empty) README."
  git push
#+end_src

Now let's pretend to be someone else working on the project. Anyone
else in the project will be able to access the copy of the repository
in the project folder. Another user would do this from their VM but
we'll just go to another directory to demonstrate.

#+begin_src bash
  cd /c/Users
  git clone /c/mnt/containers/project/Workshop_April2022/my_cool_project
#+end_src

#+begin_src bash
  cd /tmp
  git clone /c/mnt/containers/project/Workshop_April2022/my_cool_project
  cd my_cool_project
  ls
  README.md
#+end_src

<do some more demo>

* What (and why) is Docker?

Docker is a tool from the system's administrator's toolbox. It was
invented to solve at least one basic problem:

Software doesn't run in a vacuum. Any given program may depend on the
context it runs in (the operating system, libraries, environment
variables, network configuration) in non-trivial ways. Developers used
to solve this problem either haphazardly, by manually configuring
machines for development and production for each user and deployment,
or by writing complicated scripts.

When virtualization became a thing, this process was made a little
easier: you could snapshot pristine VMs at just the point you needed
them and revert or copy snapshots around to smooth out these
processes.

But virtual machines are relatively heavy-weight. Docker is a
container platform which gives you some of the fleetness of simply
running a program and most of what you'd get from maintaining virtual
machine images. It also standardizes the process of setting up virtual
machines so that VM's can be shared as built images or just files
which describe the processes.

** Docker for BACPAC Machines

The utility of Docker containers for BACPAC researchers working on the
virtual machines is this:

if you can set up your workflow on a Linux machine using pretty much
arbitrary software, then you can build a Docker container on your
local machine, upload it the BACPAC Data Portal, and run that workflow
on a VM with access to the BACPAC Data.

We have tried to anticipate your software needs, but if we failed, you
can build a Docker container to get you where you need to go.  Please
let me know if you need any help getting a Docker container for a
particular use set up. It is part of what I am here for.

Docker is a large, relatively complex, ecosystem of software. We can
only scratch the surface here, but the surface is often all a Data
Scientist or Analyst needs to exploit Docker effectively.

** Brief Introduction to Docker and Dockerfiles

A Dockerfile is just a textual-representation of the steps a user
would perform in order to configure a particular operating system for
work.

Here is a simple example of a Docker file:

#+CAPTION: A very simple Dockerfile called scilab.Dockerfile
#+INCLUDE: "./scilab.Dockerfile" src Dockerfile

We can build an image based on this Dockerfile like this:

This Dockerfile installs the scientific computing environment "scilab"
and starts a command line interpreter when run. Note that the "RUN"
lines indicate something to do on the container as part of a setup
process. You will have many RUN lines in general. The CMD line says
"when someone runs this container, do this."

#+CAPTION: Building the Docker container.
#+begin_src sh
  docker build . -f scilab.Dockerfile -t scilab
#+end_src

And then we can run it:

#+CAPTION: Building the Docker container.
#+begin_src sh
  docker run -it scilab
#+end_src

And this will start an interactive scilab shell "inside" the
container.

Its important to realize that the container isolates you from the
operating system - by default, this scilab can't see any files outside
of the container. This is actually part of the utility of containers:
they can make it harder for a hostile user from mucking around in your
host operating system.

If you have a very mature workflow, you may package source or
executables that perform an analysis directly into the container:

#+CAPTION: A very simple Dockerfile called scilab.Dockerfile
#+INCLUDE: "./scilab2.Dockerfile" src Dockerfile

#+CAPTION: Building the second Docker container.
#+begin_src sh
  docker build . -f scilab2.Dockerfile -t scilab2
  docker run -t scilab2
#+end_src

But it is much more common among Data Scientist to simply mount your
local directory as well as any data files in the container when you
run it.

#+CAPTION: Mounting our current directory /inside/ the container.
#+begin_src sh  
  docker run -v $(pwd):/home/rstudio/work -u rstudio -w /home/rstudio/work -t scilab
  --> exec("hello.sl") # note that this runs the version of hello.sl in our working directory.
#+end_src

If we were running our Docker container on our Virtual Machine, we
would also want to mount the container directory, like this:

#+CAPTION: Mounting our current directory /inside/ the container.
#+begin_src sh  
  docker run \
         -v /mnt/containers/project/Workshop_April2022/ashar_data:/mnt/containers/project/Workshop_April2022/ashar_data
         -v $(pwd):/home/rstudio/work\
         -u rstudio -w /home/rstudio/work -t scilab
  --> exec("hello.sl") # note that this runs the version of hello.sl in our working directory.
#+end_src

Speaking of which - how can we get our containers onto the VMs?

** A few technical notes

1) You will need to log out and in again once before your user is able
   to run Docker.
2) Sometimes volumes mounted inside a container will have the wrong
   permissions. You'll need to change those permissions in order to
   modify the contents of those directories.

Both these issues will hopefully resolved in a future update.

** How to get your Docker container onto the VM

In the near future we will enable a Docker registry accessible from
inside and outside of the Virtual Machines. When that happens you will
be able to push an image you have built locally to the registry and
pull it down inside your Virtual Machine.

In the meantime, however, the easiest solution is to export your
container after building it and upload it to your personal space. I'll
use a very small image as an example, since otherwise the upload
process can be challenging.

#+CAPTION: exporting and zipping up a Docker image.
#+begin_src sh
  docker pull hello-world
  docker save hello-world:latest | gzip > hello-world.tar.gz
#+end_src

#+CAPTION: Click the upload arrow in My Workspace.
#+NAME: upload-a-file-docker
[[./script-images/upload-a-file-docker.png]]

#+CAPTION: Select the hello-world.tar.gz image.
#+NAME: select-the-hello-world-image
[[./script-images/select-the-hello-world-image.png]]

On the virtual machine we simply write:

#+begin_src bash
  gunzip hello-world.tar.gz | docker image load
  docker run -t hello-world
#+end_src

** Using a Useful Dockre Container

We were asked during the initial testing phase of the Data Portal
whether we could install [[https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/][FSL]] (a suite of FMRI, MRI and DTI image
analysis tools). Setting up FSL is relatively complicated, however.

It is much easier to just grab the FSL docker [[https://hub.docker.com/r/neurodata/fsl_1604/][container]]. I've already
uploaded it to my machine on the portal. I don't know much about MRI
data analysis, but let's see if we can use FSL on the MRI data from
the Ashar paper.

* Kick Off

The repository associated with this workshop contains some analysis
and tooling which you can use to start you own visualization.

** The Workshop Repository
*** The Docker Container

The docker container that this code runs in is based on rocker/verse -
a pre-made container which includes an Rstudio installation with the
tidyverse installed along with a handful of other libraries useful for
publishing.

It also adds a set of Python libraries which I used for the analysis
(Keras, numpy, pandas, etc).

*** Makefile

The way to understand the project is to look at the Makefile, which
gives the analysis as a series of steps which depend upon one another.

For instance:

#+INCLUDE: "Makefile" src Makefile :lines "102-111"

This target generates a figure which shows the clinical outcomes from
the Ashar data set grouped by an unsupervised clustering of the
demographic data.

*** The Analysis

What is the effect of demographics on the Ashar result? Surprisingly,
while the experimenters collected demographic data, it doesn't feature
in the paper.

I performed the following analysis:

1. I used keras to train a variational auto-encoder to produce a
   two-dimensional representation of the demographic profiles of the
   study participants.
   #+CAPTION: The encoded representation of the demographic data.
   #+NAME: encoded.
   [[./script-images/demo-projection.png]]

2. From this lower dimensional representation I identified
   approximately 4-5 groups in the data and produced a clustering of
   the subjects by demographic data using Spectral Clustering (in
   sklearn).
3. Given the clustering, I was curious about what features explain
   each cluster. I trained a set of logistic predictors on cluster
   membership (using R's gbm package, which implements
   AdaBoost). These models tell us what demographic variables most
   strongly influence cluster membership.
4. I then summarized the important variables to "explain" each
   cluster.
5. And finally I grouped the clinical outcomes data by demographic
   group for comparison.
   #+CAPTION: The demographic clusters seem to correlate with response to PRT and Saline.
   #+NAME: Some people find the placebo very effective.
   [[./script-images/outcomes_by_demographic_clustering.png]]

   

The results are suggestive (for a fishing expidition, this is the best
we can hope for). Two clusters appear to account for most of the
effect of PRT whereas other groups were not as responsive.
Interestingly, one group responded very strongly to Saline, while the
other did not.
